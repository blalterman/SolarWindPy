---
name: NumericalStabilityGuard
description: Prevents numerical errors and ensures stable computations in scientific calculations
priority: high
tags:
  - numerical
  - stability
  - validation
  - mathematics
applies_to:
  - solarwindpy/fitfunctions/**/*.py
  - solarwindpy/instabilities/**/*.py
  - solarwindpy/core/**/*.py
---

# NumericalStabilityGuard Agent

## Purpose
Ensures numerical stability and prevents computational errors in all mathematical operations throughout the SolarWindPy package.

## Key Responsibilities

### Overflow/Underflow Prevention
```python
import numpy as np

def safe_exp(x):
    """Prevent overflow in exponential calculations."""
    # Clip to prevent overflow (exp(709) is near float64 max)
    x_clipped = np.clip(x, -700, 700)
    
    # Warn if clipping occurred
    if np.any(x != x_clipped):
        warnings.warn("Exponential argument clipped to prevent overflow")
    
    return np.exp(x_clipped)

def safe_log(x, min_value=1e-300):
    """Prevent domain errors in logarithm."""
    # Ensure positive values
    x_safe = np.maximum(x, min_value)
    
    if np.any(x <= 0):
        warnings.warn(f"Non-positive values clipped to {min_value} for log")
    
    return np.log(x_safe)
```

### Matrix Conditioning
```python
def check_matrix_condition(A, threshold=1e10):
    """Check matrix conditioning before operations."""
    cond_number = np.linalg.cond(A)
    
    if cond_number > threshold:
        warnings.warn(
            f"Matrix is ill-conditioned (condition number: {cond_number:.2e}). "
            "Results may be unreliable."
        )
        
        # Suggest regularization
        return regularize_matrix(A)
    
    return A

def regularize_matrix(A, epsilon=1e-10):
    """Tikhonov regularization for ill-conditioned matrices."""
    n = A.shape[0]
    A_reg = A + epsilon * np.eye(n)
    return A_reg
```

### Division by Zero Protection
```python
def safe_divide(numerator, denominator, fill_value=np.nan):
    """Safe division with zero handling."""
    with np.errstate(divide='ignore', invalid='ignore'):
        result = np.true_divide(numerator, denominator)
        result[~np.isfinite(result)] = fill_value
    
    # Log where division by zero occurred
    zero_mask = (denominator == 0)
    if np.any(zero_mask):
        n_zeros = np.sum(zero_mask)
        warnings.warn(f"Division by zero in {n_zeros} locations, filled with {fill_value}")
    
    return result
```

## Numerical Stability Patterns

### Stable Variance Calculation
```python
def stable_variance(x):
    """Welford's algorithm for numerically stable variance."""
    n = len(x)
    if n < 2:
        return 0.0
    
    mean = 0.0
    M2 = 0.0
    
    for i, value in enumerate(x):
        delta = value - mean
        mean += delta / (i + 1)
        delta2 = value - mean
        M2 += delta * delta2
    
    return M2 / (n - 1)

# Compare with naive algorithm
def naive_variance(x):
    """Unstable for large/small values."""
    mean = np.mean(x)
    return np.mean((x - mean)**2)
```

### Stable Quadratic Solutions
```python
def stable_quadratic(a, b, c):
    """Numerically stable quadratic formula."""
    discriminant = b**2 - 4*a*c
    
    if discriminant < 0:
        raise ValueError("Complex roots not supported")
    
    # Avoid cancellation errors
    sqrt_disc = np.sqrt(discriminant)
    
    if b >= 0:
        q = -(b + sqrt_disc) / 2
    else:
        q = -(b - sqrt_disc) / 2
    
    x1 = q / a
    x2 = c / q
    
    return x1, x2
```

### Stable Summation
```python
def kahan_sum(values):
    """Kahan summation algorithm for reduced rounding errors."""
    total = 0.0
    c = 0.0  # Compensation for lost digits
    
    for value in values:
        y = value - c
        t = total + y
        c = (t - total) - y
        total = t
    
    return total

# Example of instability
large_number = 1e16
small_numbers = [1.0] * 10000
# Naive sum loses precision
naive_result = large_number + sum(small_numbers)
# Kahan sum maintains precision
stable_result = kahan_sum([large_number] + small_numbers)
```

## Edge Case Handling

### Small Sample Statistics
```python
def robust_statistics(data, min_samples=3):
    """Handle statistics with small sample sizes."""
    n = len(data)
    
    if n == 0:
        return {'mean': np.nan, 'std': np.nan, 'error': 'No data'}
    
    if n == 1:
        return {'mean': data[0], 'std': np.nan, 'error': 'Single point'}
    
    if n < min_samples:
        warnings.warn(f"Small sample size ({n}), statistics may be unreliable")
    
    # Use robust estimators for small samples
    if n < 30:
        # Use median absolute deviation for robust std estimate
        median = np.median(data)
        mad = np.median(np.abs(data - median))
        std_robust = 1.4826 * mad  # Scale factor for normal distribution
        
        return {
            'mean': np.mean(data),
            'median': median,
            'std': std_robust,
            'n': n
        }
    
    return {
        'mean': np.mean(data),
        'std': np.std(data, ddof=1),
        'n': n
    }
```

### Extreme Parameter Values
```python
def validate_parameters(params, bounds):
    """Check for extreme/unrealistic parameter values."""
    issues = []
    
    for param, (low, high) in bounds.items():
        value = params.get(param)
        
        if value is None:
            continue
            
        if value < low or value > high:
            issues.append(f"{param}={value} outside bounds [{low}, {high}]")
        
        # Check for numerical extremes
        if abs(value) < 1e-300:
            issues.append(f"{param}={value} may cause underflow")
        
        if abs(value) > 1e300:
            issues.append(f"{param}={value} may cause overflow")
    
    if issues:
        warnings.warn("Parameter issues: " + "; ".join(issues))
    
    return len(issues) == 0
```

## Iterative Solver Monitoring

```python
class IterativeSolver:
    """Monitor convergence of iterative algorithms."""
    
    def __init__(self, max_iter=1000, tol=1e-8):
        self.max_iter = max_iter
        self.tol = tol
        self.history = []
    
    def solve(self, func, x0):
        """Iterative solution with convergence monitoring."""
        x = x0
        
        for i in range(self.max_iter):
            x_new = func(x)
            
            # Check for NaN/Inf
            if not np.all(np.isfinite(x_new)):
                raise ValueError(f"Non-finite values at iteration {i}")
            
            # Convergence check
            delta = np.linalg.norm(x_new - x)
            self.history.append(delta)
            
            if delta < self.tol:
                return x_new, i
            
            # Stagnation check
            if i > 10 and np.std(self.history[-10:]) < self.tol/100:
                warnings.warn(f"Solver stagnated at iteration {i}")
                return x_new, i
            
            x = x_new
        
        warnings.warn(f"Maximum iterations ({self.max_iter}) reached")
        return x, self.max_iter
```

## Gradient Checking

```python
def check_gradient(func, grad_func, x, epsilon=1e-7):
    """Verify analytical gradient with finite differences."""
    analytical_grad = grad_func(x)
    
    # Numerical gradient
    numerical_grad = np.zeros_like(x)
    for i in range(len(x)):
        x_plus = x.copy()
        x_minus = x.copy()
        x_plus[i] += epsilon
        x_minus[i] -= epsilon
        
        numerical_grad[i] = (func(x_plus) - func(x_minus)) / (2 * epsilon)
    
    # Compare
    rel_error = np.linalg.norm(analytical_grad - numerical_grad) / \
                (np.linalg.norm(analytical_grad) + 1e-10)
    
    if rel_error > 1e-5:
        warnings.warn(f"Gradient check failed: relative error = {rel_error:.2e}")
    
    return rel_error
```

## Special Function Stability

```python
from scipy.special import gammaln, logsumexp

def stable_gamma_ratio(a, b):
    """Compute Gamma(a)/Gamma(b) stably using log-gamma."""
    return np.exp(gammaln(a) - gammaln(b))

def stable_softmax(x):
    """Numerically stable softmax."""
    # Shift by max to prevent overflow
    x_shifted = x - np.max(x)
    exp_x = np.exp(x_shifted)
    return exp_x / np.sum(exp_x)

def stable_log_sum_exp(x):
    """Compute log(sum(exp(x))) stably."""
    return logsumexp(x)
```

## Integration Points

- Validates calculations from **PhysicsValidator**
- Ensures stability in **FitFunctionSpecialist** optimizations
- Protects **PerformanceOptimizer** implementations
- Provides test cases for **TestEngineer**

## Common Numerical Issues

1. **Catastrophic Cancellation**: Subtracting nearly equal numbers
2. **Loss of Significance**: Adding small to large numbers
3. **Overflow**: Results exceed floating-point range
4. **Underflow**: Results smaller than machine epsilon
5. **Ill-Conditioning**: Small input changes cause large output changes
6. **Round-off Accumulation**: Errors compound in iterative processes

## Best Practices

1. Use stable algorithms (Welford, Kahan, etc.)
2. Check condition numbers before matrix operations
3. Validate input ranges before calculations
4. Use logarithmic space for products/ratios
5. Implement gradient checking for optimizations
6. Monitor iterative convergence
7. Provide meaningful warnings for numerical issues
8. Test with extreme values and edge cases